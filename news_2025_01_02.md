# llm_notes

## notes

- attention ops
- MultiheadAttention
  https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html
- https://arxiv.org/abs/1706.03762
  Attention Is All You Need
  check paper for details, very clear
- http://arxiv.org/abs/2207.09238
  Formal Algorithms for Transformers
- LayerNorm
  https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html
  https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md
- Softmax
  https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html
  https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html
  https://stackoverflow.com/questions/49036993/pytorch-softmax-what-dimension-to-use
  understand `dim` by applying a simple demo `torch.tensor([[1,2],[3,4]])`

- Faster Attention
- http://arxiv.org/abs/2205.14135
  FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
- http://arxiv.org/abs/2307.08691
  FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
- https://arxiv.org/abs/2407.08608
  FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
- https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html
  Fused Attention
- http://arxiv.org/abs/1805.02867
  Online normalizer calculation for softmax
- http://arxiv.org/abs/2112.05682
  Self-attention Does Not Need $O(n^2)$ Memory

- opencl-intercept-layer
  https://github.com/intel/opencl-intercept-layer
  https://github.com/intel/opencl-intercept-layer/blob/main/docs/controls.md
  https://developer.codeplay.com/products/oneapi/construction-kit/4.0.0/guides/source/cl/intercept

- vLLM Paged Attention
  https://docs.vllm.ai/en/v0.6.3/dev/kernel/paged_attention.html
  or
  https://docs.vllm.ai/en/latest/design/kernel/paged_attention.html

- The Python Type System
  https://github.com/python/typing
  https://typing.readthedocs.io/en/latest/index.html
  https://typing.readthedocs.io/en/latest/spec/index.html
  https://typing.readthedocs.io/en/latest/spec/type-system.html

- AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models
  https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html
- torch.export AOTInductor Tutorial for Python runtime (Beta)
  https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html
- Profiling to understand torch.compile performance
  https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html
- Large Scale Transformer model training with Tensor Parallel (TP)
  https://pytorch.org/tutorials/intermediate/TP_tutorial.html

- Getting Started with CUDA for PyTorch
  https://tinkerd.net/blog/machine-learning/cuda-basics/
