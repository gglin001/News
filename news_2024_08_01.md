# misc_1

- https://www.together.ai/blog/together-inference-engine-2
  Announcing Together Inference Engine 2.0 with new Turbo and Lite endpoints

```
The Together inference solution is a dynamic system that continuously incorporates cutting-edge innovations from both the community and our in-house research. These advancements span various areas, which include kernels (e.g., FlashAttention-3 and FlashDecoding), models and architectures (e.g., Mamba and StripedHyena), quality-preserving quantization, speculative decoding (e.g., Medusa, Sequoia, and SpecExec), and other innovative runtime and compiler techniques. To that end, Together Inference Engine builds on these technical advancements to deliver their economic benefits directly to our customers.

Attention Kernels that Power the World of Transformer Models. Designing the fastest kernels that optimize inference for the fastest and most efficient use of hardware is one of our key technical focuses. These include, but are not limited to, matrix multiplication in low precision, mixture-of-experts, and attention decoding. Last week, in collaboration with Colfax, Meta and NVidia, we released FlashAttention-3 to the open source community, achieving up to 75% of an H100 GPU's maximum capabilities. The Together Inference Engine integrates and builds upon kernels from FlashAttention-3 along with proprietary kernels for other operators.

Speculative Decoding. The Together Inference Engine integrates not only our latest research in speculative decoding algorithms, e.g., Medusa, Sequoia, and SpecExec; it also comes with custom-built draft models for speculative decoding. These draft models are trained over a carefully designed data mixture over the RedPajama dataset, an open 30T tokens dataset that enables state-of-the-art LLMs such as XGen (Salesforce), OpenELM (Apple), Arctic (Snowflake), OLMo (AI2). We conduct training of draft models often beyond 10x Chinchilla optimal, which forms a strong foundation for high acceptance draft models that could be readily tailored for each individual customer and specific use cases.

Quality-preserving Quantization. Quality is our core focus at Together, and the Together Inference Engine 2.0 integrates our latest results of quality-preserving quantization. Specifically, we deploy a multi-dimensional accuracy preservation strategy that understands how a small precision loss from quantization can impact the end-to-end system’s speed and accuracy. With quantization techniques based on incoherence processing (from QuIP) that “spreads out” outliers to carefully balance performance and preserve quality at each individual operator level.
```

- https://siliconflow.cn/

```
Inference Layer: Cache Optimization, Scheduling Optimization, Sampling Optimization

Framework Layer: Resource Optimization, Execution Optimization, Communication Optimization, Precision Optimization
```

notes:

```
Cache Optimization: eg: k-v cache, vllm(page attention)
Scheduling Optimization: eg: In-flight batching ? Sarathi-Serve(https://arxiv.org/abs/2403.02310), In-flight batching in TensorRT-LLM?
Sampling Optimization: eg: Speculative decoding
<!--  -->
Resource Optimization:
Execution Optimization: eg: flash-attn
Communication Optimization:
Precision Optimization: eg: quantization
```

- https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
  Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog

- https://www.latent.space/p/llama-3

```
Alessio [00:09:57]: You mentioned Chinchilla is the best way to go, but then you tweeted recently, don't fall into the Chinchilla trap if you want your model to be used by billions of people. So what's the updated state of scaling loss? I think there was obviously the Kepler, and then there was Chinchilla, and then people kind of got the Llama scaling law, like the 100 to 200x parameter to token ratio. What's your updated thinking on how to think about scaling loss when you get model size and training data?

Thomas [00:10:24]: Right. So, you know, as you said, this Kepler paper with scaling laws, but they figured out, basically they tried two dimensions, the model weights and the number of training time, like number of steps, training tokens, epochs. And for that, they figured that model size is what matters. So GPT-3 was way too big compared to the actual number of training tokens because they did a mistake, not adapting the scheduler. That's what Chinchilla emphasized and discovered. To be fair, I think OpenAI knew that at the time of Chinchilla paper, but yeah, basically Chinchilla said we have to revisit the scaling laws originally published by Kepler and emphasize much more the importance of training tokens. And they did like some really good scaling laws showing that there's an optimal, basically you need to double the number of training tokens every time you double the training weights to get an optimal ratio so that for a finite number of compute, you will end with the best results in your paper. And what I call the Chinchilla trap is that, that's good if you want the best flagship model that obtains the highest performance on your paper. But if you want to use your model at inference time, inference, the two dimensions, one remains the model weights, but one drops the number of tokens you train it, number of steps. And so to be compute efficient at inference time, it's much better to train it much longer training time, even if it's an effort, an additional effort, than to have a bigger model. That's what I call, I refer to the Chinchilla trap. Not that Chinchilla was wrong, but if you can see your inference time, you need to go beyond Chinchilla. And in fact, that's what Llama1 folks did by overtraining in the sense they could have get a better performance in paper, but they prefer to create the best artifact that will be used by the community.
```

```
Swyx [00:17:48]: Okay. What should people know? What are the major choices of Llama 3 versus Llama 2?

Thomas [00:17:53]: There's not like a lot of changes in terms of architectures. I think we can do a lot better in the future and not just like with transformers, but for instance, to me, like it doesn't make sense to use the same amount of compute per token for every token. Like there's architecture lack of flexibilities. There's a lot of research to go there, but still that's the best thing we have for now. And so it's the same recipe than in terms of architectures and training than Llama 2, but we put so much effort on scaling the data and the quality of data. There's now 15 trillion tokens compared to 2 trillion. So it's another venture there as well, including for the smaller models.
```

- https://www.latent.space/p/soumith

```
PyTorch vs Tinygrad tradeoffs

Alessio [00:03:46]: Yeah, we kind of touched on PyTorch in a lot of episodes. So we had George Hotz from TinyGrad. He called PyTorch a CISC and TinyGrad a RISC. I would love to get your thoughts on PyTorch design direction as far as, I know you talk a lot about kind of having a happy path to start with and then making complexity hidden away but then available to the end user. One of the things that George mentioned is I think you have like 250 primitive operators in PyTorch, I think TinyGrad is four. So how do you think about some of the learnings that maybe he's going to run into that you already had in the past seven, eight years almost of running PyTorch?

Soumith [00:04:24]: Yeah, I think there's different models here, but I think it's two different models that people generally start with. Either they go like, I have a grand vision and I'm going to build a giant system that achieves this grand vision and maybe one is super feature complete or whatever. Or other people say they will get incrementally ambitious, right? And they say, oh, we'll start with something simple and then we'll slowly layer out complexity in a way that optimally applies Huffman coding or whatever. Like where the density of users are and what they're using, I would want to keep it in the easy, happy path and where the more niche advanced use cases, I'll still want people to try them, but they need to take additional frictional steps. George, I think just like we started with PyTorch, George started with the incrementally ambitious thing. I remember TinyGrad used to be, like we would be limited to a thousand lines of code and I think now it's at 5,000. So I think there is no real magic to which why PyTorch has the kind of complexity. I think it's probably partly necessitated and partly because we built with the technology available under us at that time, PyTorch is like 190,000 lines of code or something at this point. I think if you had to rewrite it, we would probably think about ways to rewrite it in a vastly simplified way for sure. But a lot of that complexity comes from the fact that in a very simple, explainable way, you have memory hierarchies. You have CPU has three levels of caches and then you have DRAM and SSD and then you have network. Similarly, GPU has several levels of memory and then you have different levels of network hierarchies, NVLink plus InfiniBand or Rocky or something like that, right? And the way the flops are available on your hardware, they are available in a certain way and your computation is in a certain way and you have to retrofit your computation onto both the memory hierarchy and like the flops available. When you're doing this, it is actually a fairly hard mathematical problem to do this setup, like you find the optimal thing. And finding the optimal thing is, what is optimal depends on the input variables themselves. So like, okay, what is the shape of your input tensors and what is the operation you're trying to do and various things like that. Finding that optimal configuration and writing it down in code is not the same for every input configuration you have. Like for example, just as the shape of the tensors change, let's say you have three input tensors into a Sparstar product or something like that. The shape of each of these input tensors will vastly change how you do this optimally placing this operation onto the hardware in a way that will get you maximal throughput. So a lot of our complexity comes from writing out hundreds of configurations for each single PyTorch operator and templatizing these things and symbolically generating the final CUDA code or CPU code. There's no way to avoid it because mathematically we haven't found symbolic ways to do this that also keep compile time near zero. You can write a very simple framework, but then you also should be willing to eat the long compile time. So if searching for that optimal performance at runtime, but that's the trade off. There's no, like, I don't think unless we have great breakthroughs George's vision is achievable, he should be thinking about a narrower problem such as I'm only going to make this for work for self-driving car connets or I'm only going to make this work for LLM transformers of the llama style. Like if you start narrowing the problem down, you can make a vastly simpler framework. But if you don't, if you need the generality to power all of the AI research that is happening and keep zero compile time and in all these other factors, I think it's not easy to avoid the complexity.
```

```
Swyx [00:42:47]: Stella from Eleuther sometimes says that because she has a lot of donated compute. She's trying to put it to interesting uses, but for some reason she's decided to stop making large models.

Soumith [00:42:57]: I mean, that's a cool, high conviction opinion that might pay out.

Swyx [00:43:01]: Why?

Soumith [00:43:02]: I mean, she's taking a path that most people don't care to take about in this climate and she probably will have very differentiated ideas. I mean, think about the correlation of ideas in AI right now. It's so bad, right? So everyone's fighting for the same pie. In some weird sense, that's partly why I don't really directly work on LLMs. I used to do image models and stuff and I actually stopped doing GANs because GANs were getting so hot that I didn't have any calibration of whether my work would be useful or not because, oh yeah, someone else did the same thing you did. It's like, there's so much to do, I don't understand why I need to fight for the same pie. So I think Stella's decision is very smart.
```
